#Category page has numbered pages

#For example https://fashionunited.uk/ website was used

from bs4 import BeautifulSoup
import requests
import pandas as pd

#Getting all the data from pages with articles preview
sources=[]
headers=[]
authors=[]
dates=[]

for num_page in range (1, 733):
    url=f'https://fashionunited.uk/news/fashion/page-{num_page}'
    page = requests.get(url)
    soup = BeautifulSoup(page.text, 'html.parser')
    articles = soup.findAll('div', {'class':'item column-1'})
    for article in articles:
        sources.append('https://fashionunited.uk' + article.find('a').get('href'))
        headers.append(article.find('a').text)
        authors.append(article.find('span' ,{'itemprop':'author'}).text)
        dates.append(article.find('time').get('datetime'))
    print(num_page)
    
#In sources title had new lines embedded 
newtitle=[]
for title in headers:
    new=title.replace('\n', '')
    new2=new.replace('\t','')
    newtitle.append(new2)
    
    #Scraping the text of the articles
texts=[]
for url in sources:
    page = requests.get(url)
    soup = BeautifulSoup(page.text, 'html.parser')
    articles = soup.findAll('p')
    paragraph=''
    for par in articles:
        paragraph+=par.text
    texts.append(paragraph)
    
    
data={'title':newtitle,'author':authors, 'texts':texts, 'date':dates,  'source':sources }
final_df=pd.DataFrame(data)
final_df.to_csv('fashionunited.csv')


#Category page has infinite scroll without pages links in Network

#In this case we look for all the links mentioned on the webise. The original code was taken from https://github.com/x4nth055/pythoncode-tutorials/tree/master/web-scraping/link-extractor . For example https://www.wmagazine.com/ was used

import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import colorama

colorama.init()
GREEN = colorama.Fore.GREEN
GRAY = colorama.Fore.LIGHTBLACK_EX
RESET = colorama.Fore.RESET

internal_urls = set()
external_urls = set()

def is_valid(url):
    """
    Checks whether `url` is a valid URL.
    """
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)
    
def get_all_website_links(url):
    """
    Returns all URLs that is found on `url` in which it belongs to the same website
    """
    # all URLs of `url`
    urls = set()
    # domain name of the URL without the protocol
    domain_name = urlparse(url).netloc
    soup = BeautifulSoup(requests.get(url).content, "html.parser")
    for a_tag in soup.findAll("a"):
        href = a_tag.attrs.get("href")
        if href == "" or href is None:
            # href empty tag
            continue
        # join the URL if it's relative (not absolute link)
        href = urljoin(url, href)
        parsed_href = urlparse(href)
        # remove URL GET parameters, URL fragments, etc.
        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path
        if not is_valid(href):
            # not a valid URL
            continue
        if href in internal_urls:
            # already in the set
            continue
        if domain_name not in href:
            # external link
            if href not in external_urls:
                #print(f"{GRAY}[!] External link: {href}{RESET}")
                external_urls.add(href)
            continue
        print(f"{GREEN}[*] Internal link: {href}{RESET}")
        urls.add(href)
        internal_urls.add(href)
    return urls
    
    # number of urls visited so far will be stored here
total_urls_visited = 0

def crawl(url):    
    """
    Crawls a web page and extracts all links.
    You'll find all links in `external_urls` and `internal_urls` global set variables.
    params:
        max_urls (int): number of max urls to crawl, default is 30.
    """
    global total_urls_visited
    total_urls_visited += 1
    links = get_all_website_links(url)
    for link in links:
        crawl(link)
        
#final links, just copy and paste those 

if __name__ == "__main__":
    crawl("https://www.wmagazine.com/")
    #print("[+] Total External links:", len(external_urls))
    print("[+] Total Internal links:", len(internal_urls))
    print("[+] Total:", len(external_urls) + len(internal_urls))
    
    
#Here for example we are using newspaper library to get the data. 
#Tutorial is here https://towardsdatascience.com/the-easy-way-to-web-scrape-articles-online-d28947fc5979
#Documentation is here https://newspaper.readthedocs.io/en/latest/


! pip install newspaper3k

import newspaper
from newspaper import Article
from newspaper import Source
import pandas as pd

final_df = pd.DataFrame()


for each_link in internal_urls:
    each_article = Article(each_link)
    each_article.download()
    each_article.parse()


    temp_df = pd.DataFrame(columns = ['Title', 'Authors', 'Text',
                                     'published_date', 'Source'])

  
    temp_df['Authors'] = each_article.authors
    temp_df['Title'] = each_article.title
    temp_df['Text'] = each_article.text
    temp_df['published_date'] = each_article.publish_date
    temp_df['Source'] = each_link
  
    final_df = final_df.append(temp_df, ignore_index = True)
    
final_df.to_csv('wmagazine.csv') 

    
